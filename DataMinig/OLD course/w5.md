title: Wykład 5
slug: Lecture5
category: Wyklady
date: 2018-04-19
Author: Sebastian Zajac

# klasyfikator najbliższego sąsiada kNN

**Klasyfikatory najbliższego sąsiedztwa** należą do grupy klasyfikatorów opartych na analizie przypadku tzn. nie konstruuje się w nich modelu klasyfikacyjnego a analiza dokonywana jest on-line. Metody te nazywane są często leniwymi metodami uczącymi.

Podstawowe wersje algorytmów przeznaczone są do klasyfikacji danych liczbowych. Każdy rekord zbioru treningowego to element n-wymiarowej przestrzeni wzorców. Zakładamy, że zbiór treningowy jest nie tylko zbiorem danych ale przedstawia on model klasyfikacyjny. Podstawowa wersja oznaczana jako $1NN$ przy klasyfikacji nowego rekordu $X$ wybiera obiekt $Y$ najbliższy obiektowi $X$ i przydziela mu wartość atrybutu decyzyjnego obiektu $Y$. Algorytm ten generuje duże błędy dla punktów zaszumionych i osobliwych.

Rozszerzeniem algorytmu $1NN$ jest algorytm $kNN$ - k najbliższego sąsiedztwa.

W przypadku nowego rekordu $X$ wyszukiwane jest $k$ obiektów w przestrzeni wzorców (ze zbioru treningowego) najbliższych dla $X$. Następnie wykorzystując algorytm głosowania większościowego wybierana jest klasa która dominuje w zbiorze najbliższych sąsiadów. Często też korzysta się z algorytmu ważonego kNN gdzie głosy sąsiadów mają swoje wagi.

W tej klasie algorytmów największe znaczenie ma przyjęta miara odległości. Jeśli wszystkie atrybuty są numeryczne to przestrzeń wzorców jest przestrzenią Euklidesową.

![knn]({filename}/images/rys13.png)

# Naive Bayes

 Naiwny klasyfikator Bayesowski jest prostym klasyfikatorem statystycznym modelującym relacje prawdopodobieństwa między zbiorem atrybutów a atrybutem decyzyjnym.

## Cel

Predykcja prawdopodobieństwa, że dany rekord należy do określonej klasy. Jego podstawą jest twierdzenie Bayesa.

Niech $X$ i $Y$ oznaczają parę zmiennych losowych.

$P(X,Y)$ - prawdopodobieństwo łączne zmiennych $X$ i $Y$.

$P(X = x,Y = y)$ - prawdopodobieństwo, że zmienna $X$ wynosi $x$ i zmienna $Y$ wynosi y.

Dla zmiennych niezależnych $P(X , Y) = P(X) \ast  P(Y )$.

Prawdopodobieństwo warunkowe $P(X = x|Y = y)$ - prawdopodobieństwo, że zmienna $X$ przyjmie wartość $x$ gdy wartość zmiennej $Y$ wynosi $y$.

$$P(X|Y) = \frac{P(X,Y)}{P(Y)} = \frac{P(Y|X)\ast P(X)} {P(Y)}$$

W naszym przypadku chcemy określić:
$$P(C = C_i|X)= P(X|C ={C_i)\ast P(C=C_i)} {P(X)}$$
gdzie $X = (A_1, A_2, . . . , A_n)$ oznacza krotkę dla której klasa nie jest znana.

## CEL

Wyznacz prawdopodobieństwo a posteriori $P(C = C_i|X)$ klasy $C_i$ przy znajomości klasy $X$.

Rozpatrzmy zbiór treningowy rekordów $D$ o ilości $n$. Każdy rekord $d$ to $n + 1$ wymiarowy wektor. $s_i$ to liczba rekordów zbioru $D$ należących do $C_i$ . Niech $X$ to zbiór rekordów, którego klasa nie jest znana.

Nasz cel sprowadza się do opracowania modelu klasyfikacyjnego do predykcji atrybutu decyzyjnego rekordu $X$. Jeśli wartości atrybutu decyzyjnego niedeterministycznie zależą od wartości atrybutów warunkowych, to możemy rozważać zbiór $A$ i $C$ jako zmienne losowe a zależność między nimi opisać za pomocą prawdopodobieństwa warunkowego $P(C|A)$.

$P(C = C_i|X)$ - prawdopodobieństwo a posteriori, że $C = C_i$ przy znajomości wartości atrybutów warunkowych $A$ rekordu $X$.

$P(C = C_i)$ - prawdopodobieństwo a piori, że $C = C_i$ bez wiedzy o wartościach z $A$ zastępowane estymatorem $P(C = C_i ) = \frac{s_i}{n}$. Ponieważ $P(X)$ jest stałe dla wszystkich $X$ interesuje nas tylko licznik
$P(X|C =C_i)\ast P(C =C_i)$

### Pytanie

Jak oszacować $P(X|C = C_i)$ ??

## Naiwny Bayes

Podstawowym założeniem, przyjmowanym dla algorytmu Naiwnego Bayesa jest założenie o warunkowej niezależności wartości poszczególnych atrybutów względem danej klasy $C = C_i$.

$$P(X|C=C_i)=\prod_{i=1}^{n} P(A_i = x_i|C=C_i) $$

Przyjęcie założenia o warunkowej niezależności  atrybutów uwalnia naiwny klasyfikator bayesowski od kosztownego obliczania prawdopodobieństwa $P(X|CC=_i)$ dla wszystkich kombinacji wartości atrybutów warunkowych  $A$. Obliczenia te zastępujemy oszacowaniem warunkowego prawdopodobieństwa wystąpienia wartości $x_i$ atrybuty $A_i$ dla klasy $C_i$.

## Przykład

![bayes]({filename}/images/rys14.png)

Atrybut decyzyjny **ryzyko**.

Prawdopodobieństwo a piori:

$$P(ryzyko = wysokie) = 6/14$$

$$P(ryzyko = niskie) = 8/14$$

Dla zmiennej status: $P(status = kawaler|ryzyko = wysokie) = 2/6$, 

$P(status = zonaty|ryzyko = wysokie) = 1/6$,

$P(status = kawaler|ryzyko = wysokie) = 3/6$.

Dla atrybutów ciągłych np. wiek można wybrać jedną z dwóch metod:

Podziel ciągłą zmienną na przedziały i sprawdź częstości warunkowe z poszczególnych przedziałów (20-34, 35-49,50-64). $P(wiek = 36|ryzyko = wysokie) = 3/6$

Załóż rozkład np. $N(\mi,\sigma)$ policz estymatory i na podstawie funkcji rozkładu oblicz prawdopodobieństwo.
Zadanie : $X = (36, rozwiedziony , sredni , 2)$ wyznacz ryzyko.

- Odporny na występowanie punktów osobliwych i zaszumienie danych - nie mają one istotnego wpływu na klasyfikację przy obliczaniu prawdopodobieństw warunkowych.
- Mały koszt obliczeniowy przy naiwności. W przypadku gdy założenie to nie jest spełnione można użyć tzw. sieci Bayesowskich.

# Support Vector Machine SVM

# Drzewa decyzyjne

Ciekawy sposób opisu drzewa decyzyjnego przedstawia (strona)[http://www.r2d3.us/visual-intro-to-machine-learning-part-1/].


# Lasy losowe
